{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection using GRU on tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"vectors/crawl-300d-2M-subword.vec\"\n",
    "dataset_File = \"data/wang_cleaned_full_dataset.csv\"\n",
    "traget_Emotion = \"anger\"\n",
    "max_features = 25000\n",
    "maxlen = 35\n",
    "batchsize = 250\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
    "    '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
    "    '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½',\n",
    "    'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±',\n",
    "    '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪',\n",
    "    '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã',\n",
    "    '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦',\n",
    "    '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√',\n",
    "]\n",
    "\n",
    "\n",
    "def clean_data(x):\n",
    "    \"\"\"\n",
    "    Seperates punctuations from words in given string.\n",
    "    :param x: String\n",
    "    :return: Cleaned string.\n",
    "    \"\"\"\n",
    "    x = str(x).strip().lower()\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, ' %s ' % punct)\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(target_emotion = 'anger',other_emotions=None):\n",
    "    \"\"\"\n",
    "    Prepares Train, Validation, and Test set along with the vocabulary\n",
    "    for a given target emotion.\n",
    "    :param target_emotion:\n",
    "    :param other_emotions:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset_all = pd.read_csv(dataset_File)\n",
    "    \n",
    "    # cleans up the text and makes it lower case\n",
    "    dataset_all[\"text\"] = dataset_all[\"text\"].apply(lambda x: clean_data(x))\n",
    "        \n",
    "    dataset_all[\"emotion\"] = dataset_all[\"emotion\"].apply(lambda x: clean_data(x))\n",
    "    \n",
    "    print('Number of unique tweets: {}'.format(len(dataset_all['id'].unique().tolist())))\n",
    "\n",
    "    # prints distribution of emotions in full dataset\n",
    "    s = pd.Series(dataset_all['emotion'])\n",
    "    print(s.value_counts())\n",
    "\n",
    "    # select data based on a target emotion with random selection from others\n",
    "    dataset_target = dataset_all.loc[dataset_all['emotion'] == target_emotion]\n",
    "    target_count = dataset_target['emotion'].count()\n",
    "    if other_emotions is None:\n",
    "        dataset_other = dataset_all.loc[dataset_all['emotion'] != target_emotion].sample(target_count)\n",
    "    else:\n",
    "        dataset_other = dataset_all.loc[dataset_all['emotion'] == other_emotions].sample(target_count)\n",
    "    \n",
    "    # assign float values to class labels\n",
    "    dataset_target['emotion'] = 1.0\n",
    "    dataset_other['emotion'] = 0.0\n",
    "\n",
    "    dataset = pd.concat([dataset_target, dataset_other])\n",
    "    \n",
    "    # prints distribution of emotions in selected dataset\n",
    "    s = pd.Series(dataset['emotion'])\n",
    "    print(s.value_counts())\n",
    "\n",
    "    # split to train, validation and test\n",
    "    train_df, val_test_df = train_test_split(dataset, test_size=0.2, random_state=2018)  # .08 since the datasize is large enough.\n",
    "    test_df, val_df = train_test_split(val_test_df, test_size=0.5, random_state=2018)\n",
    "\n",
    "    # prints distribution of emotions in train, validation and test sets\n",
    "    s = pd.Series(train_df['emotion'])\n",
    "    print('**************')\n",
    "    print(s.value_counts())\n",
    "    \n",
    "    s = pd.Series(test_df['emotion'])\n",
    "    print('**************')\n",
    "    print(s.value_counts())\n",
    "\n",
    "    s = pd.Series(val_df['emotion'])\n",
    "    print('**************')\n",
    "    print(s.value_counts())\n",
    "\n",
    "    # fill up the missing values\n",
    "    all_X = dataset['text'].fillna(\"_##_\").values\n",
    "    train_X = train_df[\"text\"].fillna(\"_##_\").values\n",
    "    val_X = val_df[\"text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"text\"].fillna(\"_##_\").values\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(all_X))\n",
    "    print('#### number of words: ')\n",
    "    print(tokenizer.num_words)\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    val_X = tokenizer.texts_to_sequences(val_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    all_X = tokenizer.texts_to_sequences(all_X)\n",
    "    lengths = [len(l) for l in all_X]\n",
    "\n",
    "    print('=========================================')\n",
    "\n",
    "    # plt.hist(lengths, bins = 'auto')\n",
    "    # plt.show()\n",
    "\n",
    "    # Pad the sentences. We need to pad the sequence with 0's to achieve consistent length across examples.\n",
    "    train_X = tf.keras.preprocessing.sequence.pad_sequences(train_X, maxlen=maxlen)\n",
    "    val_X = tf.keras.preprocessing.sequence.pad_sequences(val_X, maxlen=maxlen)\n",
    "    test_X = tf.keras.preprocessing.sequence.pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    # Get the target values\n",
    "    train_y = train_df['emotion'].values\n",
    "    val_y = val_df['emotion'].values\n",
    "    test_y = test_df['emotion'].values\n",
    "    print(type(train_y))\n",
    "\n",
    "    # shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "    val_idx = np.random.permutation(len(val_X))\n",
    "    tst_idx = np.random.permutation(len(test_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    val_X = val_X[val_idx]\n",
    "    test_X = test_X[tst_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    val_y = val_y[val_idx]\n",
    "    test_y = test_y[tst_idx]\n",
    "\n",
    "    return train_X, val_X, test_X, train_y, val_y, test_y, tokenizer.word_index\n",
    "\n",
    "\n",
    "def load_embedding(word_index, embedding_file):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix in which we keep only the embeddings for words which are in our word_index\n",
    "    :param word_index:\n",
    "    :param embedding_file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_file))\n",
    "    embed_size = len(embeddings_index[next(iter(embeddings_index))])\n",
    "    \n",
    "    # make sure all embeddings have the right format\n",
    "    key_to_del = []\n",
    "    for key, value in embeddings_index.items():\n",
    "        if not len(value) == embed_size:\n",
    "            key_to_del.append(key)\n",
    "    \n",
    "    for key in key_to_del:\n",
    "        del embeddings_index[key]\n",
    "\n",
    "    notFountWords = []\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "    print(\"*****embedding Size********\")\n",
    "    print(embed_size)\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            count = count + 1\n",
    "        else:\n",
    "            notFountWords.append(word)\n",
    "    \n",
    "    with open('WordsNotFound.txt', 'w') as f:\n",
    "        for item in notFountWords:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    print('# of embeding changed: ')\n",
    "    print(count)\n",
    "    return embedding_matrix, embed_size\n",
    "\n",
    "\n",
    "def model_gru(embedding_matrix, embed_size):\n",
    "    \"\"\"\n",
    "    Create model by Tensorflow.\n",
    "    :param embedding_matrix:\n",
    "    :param embed_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(maxlen,))\n",
    "    x = tf.keras.layers.Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    " \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(35, return_sequences=True))(x)\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    conc = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "    conc = tf.keras.layers.Dense(70, activation=\"relu\")(conc)\n",
    "\n",
    "    conc = tf.keras.layers.Dropout(0.5)(conc)\n",
    "    outp = tf.keras.layers.Dense(1, activation=\"sigmoid\")(conc)\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model):\n",
    "    \"\"\"\n",
    "    Train model.\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embedding_name = os.path.splitext(os.path.basename(embedding_file))[0]\n",
    "    fileName = 'weights_best' + traget_Emotion + embedding_name + '.h5'\n",
    "    filepath = fileName\n",
    "    # filepath = \"weights_best.h5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                    monitor='val_loss', verbose=2,\n",
    "                                                    save_best_only=True, mode='min')\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                                     patience=1, min_lr=0.0001, verbose=2)\n",
    "    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n",
    "                                                     patience=2, verbose=2, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr]\n",
    "\n",
    "    history = model.fit(train_X, train_y,\n",
    "                        batch_size=batchsize, epochs=num_epochs,\n",
    "                        validation_data=(val_X, val_y), callbacks=callbacks)\n",
    "    model.load_weights(filepath)\n",
    "    # plot_graphs(history, 'accuracy')\n",
    "    # plot_graphs(history, 'loss')\n",
    "    pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
    "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
    "\n",
    "    return pred_val_y, pred_test_y\n",
    "\n",
    "\n",
    "def f1_smart(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function computes the best F1 score by looking at predictions for evaluation.\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        res = metrics.f1_score(y_true, (y_pred > thresh).astype(int))\n",
    "        thresholds.append([thresh, res])\n",
    "        printout = \"F1 score at threshold {0} is {1}\".format(thresh, res)\n",
    "        print(printout)\n",
    "\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    best_f1 = thresholds[0][1]\n",
    "    print(\"Best threshold: \", best_thresh)\n",
    "    return best_f1, best_thresh\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, test_X, train_y, val_y, test_y, word_index = prepare_data(traget_Emotion)\n",
    "embedding_matrix, embedding_size = load_embedding(word_index, embedding_file)\n",
    "model1 = model_gru(embedding_matrix, embedding_size)\n",
    "print(model1.summary())\n",
    "pred_val_y, pred_test_y = train_model(model1)\n",
    "f1, threshold = f1_smart(test_y, pred_test_y)\n",
    "print(f'Optimal F1: {f1} at threshold: {threshold}')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
