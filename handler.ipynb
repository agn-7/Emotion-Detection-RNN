{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection using GRU on tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tensorflow 2 in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to your file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/gdrive/My\\ Drive/app/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/agn/PycharmProjects/Emotion-Detection-RNN\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration.cfg  requirements.txt\r\n",
      "data\t\t   test_configuration.cfg\r\n",
      "handler.ipynb\t   trained_models\r\n",
      "handler.py\t   vectors\r\n",
      "handler-test.py    weights_bestsurprisecrawl-300d-2M-subword.h5\r\n",
      "model.png\t   weights_bestsurpriseglove.6B.300d.h5\r\n",
      "README.md\t   WordsNotFound.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = \"vectors/crawl-300d-2M-subword.vec\"\n",
    "dataset_File = \"data/wang_cleaned_full_dataset.csv\"\n",
    "traget_Emotion = \"anger\"\n",
    "max_features = 25000\n",
    "maxlen = 35\n",
    "batchsize = 250\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [\n",
    "    ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
    "    '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
    "    '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½',\n",
    "    'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±',\n",
    "    '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪',\n",
    "    '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã',\n",
    "    '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦',\n",
    "    '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√',\n",
    "]\n",
    "\n",
    "\n",
    "def clean_data(x):\n",
    "    \"\"\"\n",
    "    Seperates punctuations from words in given string.\n",
    "    :param x: String\n",
    "    :return: Cleaned string.\n",
    "    \"\"\"\n",
    "    x = str(x).strip().lower()\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, ' %s ' % punct)\n",
    "    return x\n",
    "\n",
    "\n",
    "def prepare_data(target_emotion='anger', other_emotions=None):\n",
    "    \"\"\"\n",
    "    Prepares Train, Validation, and Test set along with the vocabulary\n",
    "    for a given target emotion.\n",
    "    :param target_emotion:\n",
    "    :param other_emotions:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dataset_all = pd.read_csv(dataset_File)\n",
    "    \n",
    "    # cleans up the text and makes it lower case\n",
    "    dataset_all[\"text\"] = dataset_all[\"text\"].apply(lambda x: clean_data(x))\n",
    "        \n",
    "    dataset_all[\"emotion\"] = dataset_all[\"emotion\"].apply(lambda x: clean_data(x))\n",
    "    \n",
    "    print('Number of unique tweets: {}'.format(len(dataset_all['id'].unique().tolist())))\n",
    "\n",
    "    # prints distribution of emotions in full dataset\n",
    "    s = pd.Series(dataset_all['emotion'])\n",
    "    print(s.value_counts())\n",
    "\n",
    "    # select data based on a target emotion with random selection from others\n",
    "    dataset_target = dataset_all.loc[dataset_all['emotion'] == target_emotion]\n",
    "    target_count = dataset_target['emotion'].count()\n",
    "    if other_emotions is None:\n",
    "        dataset_other = dataset_all.loc[dataset_all['emotion'] != target_emotion].sample(target_count)\n",
    "    else:\n",
    "        dataset_other = dataset_all.loc[dataset_all['emotion'] == other_emotions].sample(target_count)\n",
    "    \n",
    "    # assign float values to class labels\n",
    "    dataset_target['emotion'] = 1.0\n",
    "    dataset_other['emotion'] = 0.0\n",
    "\n",
    "    dataset = pd.concat([dataset_target, dataset_other])\n",
    "    \n",
    "    # prints distribution of emotions in selected dataset\n",
    "    s = pd.Series(dataset['emotion'])\n",
    "    print(s.value_counts())\n",
    "\n",
    "    # split to train, validation and test\n",
    "    train_df, val_test_df = train_test_split(dataset, test_size=0.2, random_state=2018)  # .08 since the datasize is large enough.\n",
    "    test_df, val_df = train_test_split(val_test_df, test_size=0.5, random_state=2018)\n",
    "\n",
    "    # prints distribution of emotions in train, validation and test sets\n",
    "    s = pd.Series(train_df['emotion'])\n",
    "    print('**************')\n",
    "    print(s.value_counts())\n",
    "    \n",
    "    s = pd.Series(test_df['emotion'])\n",
    "    print('**************')\n",
    "    print(s.value_counts())\n",
    "\n",
    "    s = pd.Series(val_df['emotion'])\n",
    "    print('**************')\n",
    "    print(s.value_counts())\n",
    "\n",
    "    # fill up the missing values\n",
    "    all_X = dataset['text'].fillna(\"_##_\").values\n",
    "    train_X = train_df[\"text\"].fillna(\"_##_\").values\n",
    "    val_X = val_df[\"text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"text\"].fillna(\"_##_\").values\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(all_X))\n",
    "    print('#### number of words: ')\n",
    "    print(tokenizer.num_words)\n",
    "    \n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    val_X = tokenizer.texts_to_sequences(val_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    all_X = tokenizer.texts_to_sequences(all_X)\n",
    "    lengths = [len(l) for l in all_X]\n",
    "\n",
    "    print('=========================================')\n",
    "\n",
    "    # plt.hist(lengths, bins = 'auto')\n",
    "    # plt.show()\n",
    "\n",
    "    # Pad the sentences. We need to pad the sequence with 0's to achieve consistent length across examples.\n",
    "    train_X = tf.keras.preprocessing.sequence.pad_sequences(train_X, maxlen=maxlen)\n",
    "    val_X = tf.keras.preprocessing.sequence.pad_sequences(val_X, maxlen=maxlen)\n",
    "    test_X = tf.keras.preprocessing.sequence.pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    # Get the target values\n",
    "    train_y = train_df['emotion'].values\n",
    "    val_y = val_df['emotion'].values\n",
    "    test_y = test_df['emotion'].values\n",
    "    print(type(train_y))\n",
    "\n",
    "    # shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "    val_idx = np.random.permutation(len(val_X))\n",
    "    tst_idx = np.random.permutation(len(test_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    val_X = val_X[val_idx]\n",
    "    test_X = test_X[tst_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    val_y = val_y[val_idx]\n",
    "    test_y = test_y[tst_idx]\n",
    "\n",
    "    return train_X, val_X, test_X, train_y, val_y, test_y, tokenizer.word_index\n",
    "\n",
    "\n",
    "def load_embedding(word_index, embedding_file):\n",
    "    \"\"\"\n",
    "    Create an embedding matrix in which we keep only the embeddings for words which are in our word_index\n",
    "    :param word_index:\n",
    "    :param embedding_file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_file))\n",
    "    embed_size = len(embeddings_index[next(iter(embeddings_index))])\n",
    "    \n",
    "    # make sure all embeddings have the right format\n",
    "    key_to_del = []\n",
    "    for key, value in embeddings_index.items():\n",
    "        if not len(value) == embed_size:\n",
    "            key_to_del.append(key)\n",
    "    \n",
    "    for key in key_to_del:\n",
    "        del embeddings_index[key]\n",
    "\n",
    "    notFountWords = []\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "    print(\"*****embedding Size********\")\n",
    "    print(embed_size)\n",
    "    # word_index = tokenizer.word_index\n",
    "    print(f\"word_index length: {len(word_index)}\\n\")\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            count += 1\n",
    "        else:\n",
    "            notFountWords.append(word)\n",
    "    \n",
    "    # with open('WordsNotFound.txt', 'w') as f:\n",
    "    #     for item in notFountWords:\n",
    "    #         f.write(\"%s\\n\" % item)\n",
    "\n",
    "    print('# of embeding changed: ')\n",
    "    print(count)\n",
    "    return embedding_matrix, embed_size\n",
    "\n",
    "\n",
    "def model_gru(embedding_matrix, embed_size):\n",
    "    \"\"\"\n",
    "    Create model by Tensorflow.\n",
    "    :param embedding_matrix:\n",
    "    :param embed_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    inp = tf.keras.layers.Input(shape=(maxlen,))\n",
    "    x = tf.keras.layers.Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    " \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(35, return_sequences=True))(x)\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "    conc = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "    conc = tf.keras.layers.Dense(70, activation=\"relu\")(conc)\n",
    "\n",
    "    conc = tf.keras.layers.Dropout(0.5)(conc)\n",
    "    outp = tf.keras.layers.Dense(1, activation=\"sigmoid\")(conc)\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model):\n",
    "    \"\"\"\n",
    "    Train model.\n",
    "    :param model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # embedding_name = os.path.splitext(os.path.basename(embedding_file))[0]\n",
    "    # fileName = 'weights_best' + traget_Emotion + embedding_name + '.h5'\n",
    "    # filepath = fileName\n",
    "    # # filepath = \"weights_best.h5\"\n",
    "    # checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath,\n",
    "    #                                                 monitor='val_loss', verbose=2,\n",
    "    #                                                 save_best_only=True, mode='min')\n",
    "    # reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "    #                                                  patience=1, min_lr=0.0001, verbose=2)\n",
    "    # earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n",
    "    #                                                  patience=2, verbose=2, mode='auto')\n",
    "    # callbacks = [checkpoint, reduce_lr]\n",
    "\n",
    "    history = model.fit(train_X, train_y,\n",
    "                        batch_size=batchsize, epochs=num_epochs,\n",
    "                        validation_data=(val_X, val_y))  # , callbacks=callbacks)\n",
    "    # model.load_weights(filepath)\n",
    "    # plot_graphs(history, 'accuracy')\n",
    "    # plot_graphs(history, 'loss')\n",
    "    pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n",
    "    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
    "\n",
    "    return pred_val_y, pred_test_y\n",
    "\n",
    "\n",
    "def f1_smart(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function computes the best F1 score by looking at predictions for evaluation.\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    thresholds = []\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        res = metrics.f1_score(y_true, (y_pred > thresh).astype(int))\n",
    "        thresholds.append([thresh, res])\n",
    "        printout = \"F1 score at threshold {0} is {1}\".format(thresh, res)\n",
    "        print(printout)\n",
    "\n",
    "    thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_thresh = thresholds[0][0]\n",
    "    best_f1 = thresholds[0][1]\n",
    "    print(\"Best threshold: \", best_thresh)\n",
    "    return best_f1, best_thresh\n",
    "\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tweets: 1387787\n",
      "joy             393631\n",
      "sadness         338015\n",
      "anger           298480\n",
      "love            169267\n",
      "thankfulness     79341\n",
      "fear             73575\n",
      "nan              23349\n",
      "surprise         13535\n",
      "Name: emotion, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agn/.local/lib/python3.6/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    298480\n",
      "0.0    298480\n",
      "Name: emotion, dtype: int64\n",
      "**************\n",
      "1.0    238958\n",
      "0.0    238610\n",
      "Name: emotion, dtype: int64\n",
      "**************\n",
      "0.0    29919\n",
      "1.0    29777\n",
      "Name: emotion, dtype: int64\n",
      "**************\n",
      "0.0    29951\n",
      "1.0    29745\n",
      "Name: emotion, dtype: int64\n",
      "#### number of words: \n",
      "25000\n",
      "=========================================\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_X, val_X, test_X, train_y, val_y, test_y, word_index = prepare_data(traget_Emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agn/.local/lib/python3.6/site-packages/ipykernel_launcher.py:153: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****embedding Size********\n",
      "300\n",
      "# of embeding changed: \n",
      "23506\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, embedding_size = load_embedding(word_index, embedding_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 35)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 35, 300)      7500000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 35, 70)       70770       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 70)           0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 70)           0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 140)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 70)           9870        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 70)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            71          dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,580,711\n",
      "Trainable params: 80,711\n",
      "Non-trainable params: 7,500,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 477568 samples, validate on 59696 samples\n",
      "Epoch 1/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.4790 - accuracy: 0.7660\n",
      "Epoch 00001: val_loss improved from inf to 0.43206, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "477568/477568 [==============================] - 267s 559us/sample - loss: 0.4790 - accuracy: 0.7660 - val_loss: 0.4321 - val_accuracy: 0.7952\n",
      "Epoch 2/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8037\n",
      "Epoch 00002: val_loss improved from 0.43206 to 0.41308, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "477568/477568 [==============================] - 304s 637us/sample - loss: 0.4226 - accuracy: 0.8037 - val_loss: 0.4131 - val_accuracy: 0.8062\n",
      "Epoch 3/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8136\n",
      "Epoch 00003: val_loss improved from 0.41308 to 0.40273, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "477568/477568 [==============================] - 378s 792us/sample - loss: 0.4047 - accuracy: 0.8136 - val_loss: 0.4027 - val_accuracy: 0.8114\n",
      "Epoch 4/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.8188\n",
      "Epoch 00004: val_loss improved from 0.40273 to 0.39713, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "477568/477568 [==============================] - 327s 686us/sample - loss: 0.3944 - accuracy: 0.8188 - val_loss: 0.3971 - val_accuracy: 0.8145\n",
      "Epoch 5/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.8230\n",
      "Epoch 00005: val_loss improved from 0.39713 to 0.39211, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "477568/477568 [==============================] - 247s 518us/sample - loss: 0.3866 - accuracy: 0.8230 - val_loss: 0.3921 - val_accuracy: 0.8152\n",
      "Epoch 6/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.8270\n",
      "Epoch 00006: val_loss improved from 0.39211 to 0.39051, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "477568/477568 [==============================] - 230s 482us/sample - loss: 0.3796 - accuracy: 0.8270 - val_loss: 0.3905 - val_accuracy: 0.8175\n",
      "Epoch 7/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3740 - accuracy: 0.8300\n",
      "Epoch 00007: val_loss improved from 0.39051 to 0.38674, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "477568/477568 [==============================] - 230s 482us/sample - loss: 0.3740 - accuracy: 0.8300 - val_loss: 0.3867 - val_accuracy: 0.8192\n",
      "Epoch 8/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3688 - accuracy: 0.8323\n",
      "Epoch 00008: val_loss did not improve from 0.38674\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "477568/477568 [==============================] - 242s 507us/sample - loss: 0.3688 - accuracy: 0.8323 - val_loss: 0.3925 - val_accuracy: 0.8194\n",
      "Epoch 9/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3590 - accuracy: 0.8375\n",
      "Epoch 00009: val_loss did not improve from 0.38674\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "477568/477568 [==============================] - 288s 603us/sample - loss: 0.3590 - accuracy: 0.8375 - val_loss: 0.3874 - val_accuracy: 0.8211\n",
      "Epoch 10/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8407\n",
      "Epoch 00010: val_loss improved from 0.38674 to 0.38672, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "477568/477568 [==============================] - 261s 547us/sample - loss: 0.3527 - accuracy: 0.8407 - val_loss: 0.3867 - val_accuracy: 0.8209\n",
      "Epoch 11/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3490 - accuracy: 0.8427\n",
      "Epoch 00011: val_loss improved from 0.38672 to 0.38630, saving model to weights_bestangercrawl-300d-2M-subword.h5\n",
      "477568/477568 [==============================] - 259s 543us/sample - loss: 0.3491 - accuracy: 0.8427 - val_loss: 0.3863 - val_accuracy: 0.8210\n",
      "Epoch 12/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3479 - accuracy: 0.8433\n",
      "Epoch 00012: val_loss did not improve from 0.38630\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "477568/477568 [==============================] - 255s 533us/sample - loss: 0.3479 - accuracy: 0.8433 - val_loss: 0.3867 - val_accuracy: 0.8212\n",
      "Epoch 13/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3467 - accuracy: 0.8439\n",
      "Epoch 00013: val_loss did not improve from 0.38630\n",
      "477568/477568 [==============================] - 278s 582us/sample - loss: 0.3467 - accuracy: 0.8439 - val_loss: 0.3867 - val_accuracy: 0.8215\n",
      "Epoch 14/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3457 - accuracy: 0.8441\n",
      "Epoch 00014: val_loss did not improve from 0.38630\n",
      "477568/477568 [==============================] - 288s 604us/sample - loss: 0.3457 - accuracy: 0.8441 - val_loss: 0.3877 - val_accuracy: 0.8216\n",
      "Epoch 15/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3450 - accuracy: 0.8445\n",
      "Epoch 00015: val_loss did not improve from 0.38630\n",
      "477568/477568 [==============================] - 294s 615us/sample - loss: 0.3450 - accuracy: 0.8445 - val_loss: 0.3881 - val_accuracy: 0.8217\n",
      "Epoch 16/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy: 0.8448\n",
      "Epoch 00016: val_loss did not improve from 0.38630\n",
      "477568/477568 [==============================] - 282s 590us/sample - loss: 0.3445 - accuracy: 0.8448 - val_loss: 0.3886 - val_accuracy: 0.8207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.8451\n",
      "Epoch 00017: val_loss did not improve from 0.38630\n",
      "477568/477568 [==============================] - 280s 587us/sample - loss: 0.3439 - accuracy: 0.8451 - val_loss: 0.3877 - val_accuracy: 0.8217\n",
      "Epoch 18/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3431 - accuracy: 0.8455\n",
      "Epoch 00018: val_loss did not improve from 0.38630\n",
      "477568/477568 [==============================] - 253s 531us/sample - loss: 0.3431 - accuracy: 0.8455 - val_loss: 0.3897 - val_accuracy: 0.8217\n",
      "Epoch 19/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3429 - accuracy: 0.8455\n",
      "Epoch 00019: val_loss did not improve from 0.38630\n",
      "477568/477568 [==============================] - 263s 551us/sample - loss: 0.3429 - accuracy: 0.8455 - val_loss: 0.3899 - val_accuracy: 0.8206\n",
      "Epoch 20/20\n",
      "477500/477568 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.8459\n",
      "Epoch 00020: val_loss did not improve from 0.38630\n",
      "477568/477568 [==============================] - 258s 540us/sample - loss: 0.3421 - accuracy: 0.8459 - val_loss: 0.3892 - val_accuracy: 0.8211\n",
      "F1 score at threshold 0.1 is 0.7743996183003764\n",
      "F1 score at threshold 0.11 is 0.7781752175537877\n",
      "F1 score at threshold 0.12 is 0.7820252517782468\n",
      "F1 score at threshold 0.13 is 0.7850163748071561\n",
      "F1 score at threshold 0.14 is 0.7882195958825771\n",
      "F1 score at threshold 0.15 is 0.7911780821917808\n",
      "F1 score at threshold 0.16 is 0.7938125869502335\n",
      "F1 score at threshold 0.17 is 0.7961275310933216\n",
      "F1 score at threshold 0.18 is 0.7982391374002201\n",
      "F1 score at threshold 0.19 is 0.8009642742014604\n",
      "F1 score at threshold 0.2 is 0.803031370173684\n",
      "F1 score at threshold 0.21 is 0.8046208077919504\n",
      "F1 score at threshold 0.22 is 0.8069066632503451\n",
      "F1 score at threshold 0.23 is 0.8084156928930017\n",
      "F1 score at threshold 0.24 is 0.8103078847840781\n",
      "F1 score at threshold 0.25 is 0.8122578779994217\n",
      "F1 score at threshold 0.26 is 0.813639070010601\n",
      "F1 score at threshold 0.27 is 0.8154506437768241\n",
      "F1 score at threshold 0.28 is 0.8168633668017145\n",
      "F1 score at threshold 0.29 is 0.8180007964954202\n",
      "F1 score at threshold 0.3 is 0.8193696700037078\n",
      "F1 score at threshold 0.31 is 0.8204309894862426\n",
      "F1 score at threshold 0.32 is 0.8218167643663513\n",
      "F1 score at threshold 0.33 is 0.8223330370459647\n",
      "F1 score at threshold 0.34 is 0.8233779608650876\n",
      "F1 score at threshold 0.35 is 0.8240327664174673\n",
      "F1 score at threshold 0.36 is 0.825020666850372\n",
      "F1 score at threshold 0.37 is 0.8253802105781665\n",
      "F1 score at threshold 0.38 is 0.8255687973997833\n",
      "F1 score at threshold 0.39 is 0.8262175665789842\n",
      "F1 score at threshold 0.4 is 0.8267572347518507\n",
      "F1 score at threshold 0.41 is 0.8274982683710094\n",
      "F1 score at threshold 0.42 is 0.8279367291036623\n",
      "F1 score at threshold 0.43 is 0.8274873819797155\n",
      "F1 score at threshold 0.44 is 0.8275806813447426\n",
      "F1 score at threshold 0.45 is 0.8274339847588973\n",
      "F1 score at threshold 0.46 is 0.8270094169894809\n",
      "F1 score at threshold 0.47 is 0.8262927657351788\n",
      "F1 score at threshold 0.48 is 0.8255747079218495\n",
      "F1 score at threshold 0.49 is 0.8255076877244666\n",
      "F1 score at threshold 0.5 is 0.824354956997133\n",
      "Best threshold:  0.42\n",
      "Optimal F1: 0.8279367291036623 at threshold: 0.42\n"
     ]
    }
   ],
   "source": [
    "model1 = model_gru(embedding_matrix, embedding_size)\n",
    "print(model1.summary())\n",
    "pred_val_y, pred_test_y = train_model(model1)\n",
    "f1, threshold = f1_smart(test_y, pred_test_y)\n",
    "print(f'Optimal F1: {f1} at threshold: {threshold}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
